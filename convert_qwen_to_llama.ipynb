{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-7B-Instruct\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load weights: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "qwen2_dir = Path(\"/root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c\")\n",
    "\n",
    "qwen2_state_dict = dict()\n",
    "for filepath in tqdm(Path(qwen2_dir).glob(\"*.safetensors\"), desc=\"Load weights\"):\n",
    "    with safe_open(filepath, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for key in f.keys():\n",
    "            qwen2_state_dict[key] = f.get_tensor(key)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.4219e-01,  2.2969e+00,  5.6641e-01, -2.9297e-01,  5.3516e-01,\n",
       "         4.8438e-01,  2.3906e+00,  1.0312e+00,  2.6367e-01, -6.6406e-01,\n",
       "         6.2109e-01,  5.3516e-01,  3.4062e+00, -9.6094e-01, -8.3203e-01,\n",
       "        -2.6758e-01, -1.0703e+00,  5.7812e-01,  8.3203e-01,  3.9648e-01,\n",
       "         1.3965e-01,  8.6914e-02,  4.5625e+00, -1.9141e-01,  5.6641e-01,\n",
       "         8.9844e-01,  1.0645e-01, -4.7656e-01, -3.5938e-01,  3.2617e-01,\n",
       "         8.8281e-01,  3.5625e+00, -2.9883e-01, -4.6875e-01, -4.8438e-01,\n",
       "         3.9648e-01, -5.1172e-01,  1.1406e+00, -1.6406e+00,  7.0312e-01,\n",
       "         8.0078e-01, -8.0625e+00, -1.4766e+00, -1.0781e+00, -3.1250e+00,\n",
       "        -1.3672e+00, -3.4766e-01, -1.5000e+00, -7.5391e-01,  9.7500e+00,\n",
       "        -2.2070e-01, -1.4438e+01, -1.3250e+01, -2.0000e+01,  1.8000e+01,\n",
       "        -6.7500e+01,  7.1500e+01,  1.0150e+02,  7.5000e+01,  6.8000e+01,\n",
       "         7.4500e+01,  1.2700e+02, -1.0150e+02,  1.6600e+02, -4.4375e+00,\n",
       "        -3.8086e-01,  8.5938e-01, -1.9219e+00, -1.2188e+00, -1.2344e+00,\n",
       "         7.4219e-01, -1.1230e-02, -2.2754e-01,  5.0781e-01, -9.2969e-01,\n",
       "         4.7266e-01,  7.6562e-01, -9.3750e-01, -3.8086e-01,  3.4668e-02,\n",
       "        -3.9453e-01,  6.8359e-01,  1.6113e-02,  2.6406e+00,  5.5078e-01,\n",
       "         2.0312e-01, -1.9141e+00,  6.6528e-03,  7.2656e-01, -6.3672e-01,\n",
       "        -6.9336e-02,  2.5195e-01,  9.6875e-01, -1.0596e-01, -1.0781e+00,\n",
       "         4.6875e+00,  8.6426e-02,  1.3438e+00, -7.5391e-01, -9.3750e-01,\n",
       "         1.4062e+00, -9.9609e-01, -2.2070e-01, -4.1992e-01, -9.7656e-01,\n",
       "         1.3000e+01, -9.0625e-01, -1.6250e+00,  3.4375e+00, -5.0000e-01,\n",
       "        -1.4922e+00, -4.2500e+00, -3.0938e+00,  8.9375e+00, -1.7891e+00,\n",
       "        -1.5000e+00,  1.4625e+01, -9.8750e+00, -4.4000e+01,  1.6125e+01,\n",
       "         8.2000e+01,  6.7000e+01, -3.3750e+00,  1.5800e+02,  1.5500e+02,\n",
       "        -1.1050e+02, -1.6000e+02, -1.0900e+02,  7.0312e+00, -2.1406e+00,\n",
       "         3.4219e+00, -2.4062e+00, -2.9844e+00,  2.5781e-01,  7.1875e-01,\n",
       "         1.3184e-01, -4.3750e-01, -2.2656e-01,  3.2344e+00,  7.3438e-01,\n",
       "         8.0566e-03,  3.2031e+00, -5.1562e-01, -2.1191e-01,  1.5723e-01,\n",
       "         1.9629e-01, -8.0469e-01,  5.6885e-02,  1.8594e+00,  2.6758e-01,\n",
       "         6.5234e-01, -8.7500e-01, -3.5312e+00, -2.1387e-01, -4.3213e-02,\n",
       "         3.8672e-01,  3.3203e-01,  1.6406e-01, -5.1953e-01, -1.1641e+00,\n",
       "        -5.2490e-02,  4.6875e+00, -3.7031e+00,  3.8330e-02,  2.3242e-01,\n",
       "        -1.1094e+00, -1.0205e-01,  4.5117e-01, -7.4219e-01,  1.6016e-01,\n",
       "         9.0625e-01,  9.2578e-01,  9.5312e-01, -1.8750e-01, -1.2734e+00,\n",
       "        -2.3730e-01,  8.2812e-01,  1.3203e+00, -8.4375e-01,  7.3828e-01,\n",
       "         4.9805e-01, -4.6250e+00, -2.9062e+00, -6.1250e+01,  1.3672e+00,\n",
       "        -1.2500e+01,  3.7250e+01,  2.9375e+01,  6.7000e+01,  1.0400e+02,\n",
       "        -1.0250e+02,  3.6250e+01, -1.8906e+00,  1.1641e+00,  1.1816e-01,\n",
       "         9.3750e-01,  1.7656e+00, -8.5547e-01,  6.0547e-01, -2.3438e+00,\n",
       "         1.1953e+00,  1.5869e-02, -1.9141e-01, -5.1172e-01,  2.0020e-01,\n",
       "        -7.0312e-01,  2.5586e-01, -6.1719e-01, -2.7734e-01, -3.6719e-01,\n",
       "        -5.2734e-01, -4.3945e-02,  3.0000e+00,  1.5918e-01, -2.4219e-01,\n",
       "        -5.0781e-01,  2.0781e+00,  1.6797e-01,  4.0039e-01, -7.0312e-01,\n",
       "         7.2266e-01,  5.9766e-01, -6.6895e-02, -2.3535e-01, -3.3984e-01,\n",
       "        -2.4688e+00, -3.2188e+00, -1.1865e-01, -5.7422e-01,  2.2754e-01,\n",
       "        -1.1250e+00,  3.0518e-02, -7.2754e-02,  2.8516e-01,  9.4922e-01,\n",
       "        -1.0469e+00, -1.5078e+00, -4.0938e+00,  1.4609e+00, -9.2773e-02,\n",
       "        -2.0000e+00, -1.5234e+00, -1.6484e+00,  1.1133e-01, -8.6719e-01,\n",
       "         1.4141e+00, -2.3750e+00, -1.9750e+01, -9.6250e+00,  5.4062e+00,\n",
       "        -1.7500e+01, -5.4750e+01,  7.8500e+01, -3.0375e+01, -1.2200e+02,\n",
       "         9.6000e+01,  2.5156e+00, -1.6641e+00,  1.9531e+00,  6.8359e-01,\n",
       "         1.8906e+00, -1.4844e-01,  3.8086e-01, -1.6016e-01,  1.7676e-01,\n",
       "        -6.9922e-01,  9.7656e-02, -7.0703e-01, -4.1406e-01,  3.0762e-02,\n",
       "         6.2988e-02,  9.8633e-02, -2.4414e-01, -9.8145e-02,  2.5312e+00,\n",
       "        -8.3984e-02,  6.1035e-03, -2.4512e-01, -4.5508e-01, -2.7539e-01,\n",
       "         7.2266e-01,  2.3242e-01, -1.7656e+00, -6.7969e-01, -2.4414e-01,\n",
       "         5.5078e-01,  9.9219e-01, -5.3125e-01, -4.6875e-02,  2.2070e-01,\n",
       "        -1.0938e+00, -7.8906e-01,  2.3145e-01, -2.6172e-01,  5.8203e-01,\n",
       "         2.3594e+00,  6.2500e-02,  2.0703e-01, -2.0781e+00, -3.2422e-01,\n",
       "         7.1484e-01, -5.1562e-01, -1.1035e-01,  3.5547e-01, -2.0703e-01,\n",
       "        -1.8066e-01, -3.4180e-02,  9.5625e+00,  7.5000e+00, -1.6504e-01,\n",
       "         3.2031e-01, -1.6094e+00,  1.3047e+00, -1.0562e+01, -4.2250e+01,\n",
       "        -8.0000e+00,  5.7250e+01,  2.5375e+01, -9.0000e+01, -6.6000e+01,\n",
       "         1.9989e-03,  1.5078e+00, -7.1777e-02,  5.2344e-01, -4.2188e-01,\n",
       "         3.0859e-01, -1.9375e+00, -1.5078e+00, -5.8984e-01,  3.5156e-01,\n",
       "        -2.1562e+00,  9.0332e-02,  2.7466e-02,  1.0703e+00, -1.3672e+00,\n",
       "        -2.9297e-01, -6.7969e-01,  1.9336e-01, -6.6406e-01, -7.2656e-01,\n",
       "         1.1084e-01,  2.2656e-01, -5.3516e-01, -1.3359e+00, -2.4707e-01,\n",
       "        -1.2256e-01, -2.9062e+00, -5.0391e-01, -8.5938e-01, -3.7109e-01,\n",
       "        -8.9453e-01, -4.4336e-01, -3.8672e-01, -4.0625e-01, -4.6484e-01,\n",
       "        -1.2891e+00,  4.0820e-01, -8.5449e-02,  3.7695e-01, -5.3438e+00,\n",
       "         3.0518e-02, -8.9111e-03,  2.0625e+00, -4.2969e-02, -1.4551e-01,\n",
       "        -4.9609e-01,  2.2266e-01,  7.5000e-01,  7.3730e-02, -2.3633e-01,\n",
       "         9.2578e-01,  6.8750e+00,  1.2250e+01,  1.4219e+00, -5.5469e-01,\n",
       "         9.7266e-01, -2.5938e+00,  4.8438e+00, -1.1562e+01,  7.8000e+01,\n",
       "         1.8875e+01, -1.0300e+02,  7.9500e+01,  1.0400e+02, -3.6719e-01,\n",
       "        -1.7944e-02, -1.4531e+00,  1.2578e+00,  1.6602e-02,  1.8359e+00,\n",
       "         1.0498e-02,  1.7090e-02, -1.5234e+00, -2.6953e-01, -1.6406e-01,\n",
       "         1.1035e-01,  6.4062e-01,  1.9238e-01, -2.1191e-01, -9.0332e-02,\n",
       "        -6.9531e-01, -7.4707e-02,  7.9590e-02,  1.3977e-02, -2.1875e+00,\n",
       "         6.9824e-02,  4.1504e-02,  9.0332e-02,  2.2266e-01,  1.7480e-01,\n",
       "         1.0312e+00, -2.1484e-02,  1.7969e-01, -1.7578e-02, -1.0107e-01,\n",
       "         3.9368e-03,  6.2188e+00,  3.4180e-02, -1.5723e-01,  4.2188e-01,\n",
       "        -4.2480e-02,  7.9102e-02, -8.0078e-01,  1.5918e-01,  1.6211e-01,\n",
       "         5.9375e-01,  1.6953e+00, -1.2344e+00,  2.7812e+00,  2.9375e+00,\n",
       "        -1.3672e+00,  1.7188e+00,  5.0781e-01,  3.3750e+00, -7.7188e+00,\n",
       "         1.0375e+01,  3.6719e-01,  7.9000e+01, -7.1562e+00,  2.1000e+01,\n",
       "        -2.0156e+00,  6.6562e+00,  2.5938e+00, -4.5898e-01,  3.5000e+00,\n",
       "         5.3125e+00, -8.0625e+00, -5.1562e+00,  1.8750e+00,  1.4219e+00,\n",
       "         3.9062e-01, -1.2734e+00,  4.1748e-02,  8.4375e-01, -4.5410e-02,\n",
       "         1.4258e-01, -7.5781e-01,  4.8584e-02, -6.8848e-02, -2.8906e-01,\n",
       "         4.7656e-01,  1.5625e-01,  3.4570e-01, -1.0925e-02, -8.0078e-02,\n",
       "        -3.2959e-02,  7.8613e-02,  1.0681e-02, -2.7812e+00, -3.8330e-02,\n",
       "        -1.2256e-01,  1.6211e-01, -3.5156e-02, -1.6797e-01, -8.8672e-01,\n",
       "        -1.0156e-01,  9.4727e-02, -3.0078e-01,  3.3691e-02, -1.6479e-02,\n",
       "         5.1250e+00,  9.1309e-02, -1.8066e-01,  1.1426e-01,  2.1484e-01,\n",
       "         3.8281e-01, -9.1016e-01,  8.2031e-01,  9.7656e-02, -2.0117e-01,\n",
       "        -6.1719e-01, -2.9844e+00,  6.8359e-01,  7.8125e-01, -8.9844e-02,\n",
       "         1.5938e+00, -3.4688e+00, -6.8438e+00,  6.1250e+00, -8.5000e+00,\n",
       "        -2.1719e+00, -5.4500e+01,  1.6641e+00,  4.2812e+00, -1.7375e+01,\n",
       "         2.9844e+00, -6.6250e+00, -2.9062e+00,  6.6250e+00,  2.2812e+00,\n",
       "        -4.4062e+00,  7.2500e+00], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen2_state_dict[\"model.layers.0.self_attn.k_proj.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm_head.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.embed_tokens.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.k_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.q_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.v_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.k_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.q_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.v_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.10.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.14.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.14.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.14.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.2.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.k_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.q_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.v_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.20.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.input_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.22.mlp.down_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.22.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.post_attention_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.k_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.q_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.v_proj.bias': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.input_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.mlp.down_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.mlp.gate_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.mlp.up_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.post_attention_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.k_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.k_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.o_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.q_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.q_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.v_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.v_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.input_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.mlp.down_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.mlp.gate_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.mlp.up_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.post_attention_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.k_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.k_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.o_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.q_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.q_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.v_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.v_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.input_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.mlp.down_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.mlp.gate_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.mlp.up_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.post_attention_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.k_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.k_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.o_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.q_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.q_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.v_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.v_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.input_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.mlp.down_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.mlp.gate_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.mlp.up_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.post_attention_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.k_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.k_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.o_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.q_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.q_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.v_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.v_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.input_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.mlp.down_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.mlp.gate_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.mlp.up_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.post_attention_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.k_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.k_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.o_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.q_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.q_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.v_proj.bias': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.v_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.3.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.k_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.q_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.v_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.k_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.q_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.v_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.k_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.q_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.v_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.6.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.6.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.6.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.6.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.k_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.q_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.v_proj.bias': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.k_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.q_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.v_proj.bias': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.norm.weight': 'model-00004-of-00004.safetensors'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "qwen2_dir = Path(\"/root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c\")\n",
    "with open(qwen2_dir / \"model.safetensors.index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    index_qwen2 = json.load(f)\n",
    "\n",
    "index_qwen2[\"weight_map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm_head.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.embed_tokens.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.0.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.1.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.10.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.10.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.11.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.12.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.13.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.14.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.15.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.16.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.17.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.18.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.19.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.2.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.2.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.20.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.20.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.20.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.21.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.21.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.22.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.23.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.24.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.25.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.26.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.27.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.28.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.29.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.3.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.3.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.30.input_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.mlp.down_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.post_attention_layernorm.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.30.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.31.input_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.31.mlp.down_proj.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.31.mlp.gate_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.31.mlp.up_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.31.post_attention_layernorm.weight': 'model-00004-of-00004.safetensors',\n",
       " 'model.layers.31.self_attn.k_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.31.self_attn.o_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.31.self_attn.q_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.31.self_attn.v_proj.weight': 'model-00003-of-00004.safetensors',\n",
       " 'model.layers.4.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.4.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.5.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.6.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.7.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.input_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.mlp.down_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.mlp.gate_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.mlp.up_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.post_attention_layernorm.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.k_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.o_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.q_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.8.self_attn.v_proj.weight': 'model-00001-of-00004.safetensors',\n",
       " 'model.layers.9.input_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.mlp.down_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.mlp.gate_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.mlp.up_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.post_attention_layernorm.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.k_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.o_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.q_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.layers.9.self_attn.v_proj.weight': 'model-00002-of-00004.safetensors',\n",
       " 'model.norm.weight': 'model-00004-of-00004.safetensors'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_dir = Path(\"/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\")\n",
    "with open(llama3_dir / \"model.safetensors.index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    index_llama3 = json.load(f)\n",
    "\n",
    "index_llama3[\"weight_map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.layers.0.self_attn.k_proj.bias',\n",
       " 'model.layers.0.self_attn.q_proj.bias',\n",
       " 'model.layers.0.self_attn.v_proj.bias',\n",
       " 'model.layers.1.self_attn.k_proj.bias',\n",
       " 'model.layers.1.self_attn.q_proj.bias',\n",
       " 'model.layers.1.self_attn.v_proj.bias',\n",
       " 'model.layers.10.self_attn.k_proj.bias',\n",
       " 'model.layers.10.self_attn.q_proj.bias',\n",
       " 'model.layers.10.self_attn.v_proj.bias',\n",
       " 'model.layers.11.self_attn.k_proj.bias',\n",
       " 'model.layers.11.self_attn.q_proj.bias',\n",
       " 'model.layers.11.self_attn.v_proj.bias',\n",
       " 'model.layers.12.self_attn.k_proj.bias',\n",
       " 'model.layers.12.self_attn.q_proj.bias',\n",
       " 'model.layers.12.self_attn.v_proj.bias',\n",
       " 'model.layers.13.self_attn.k_proj.bias',\n",
       " 'model.layers.13.self_attn.q_proj.bias',\n",
       " 'model.layers.13.self_attn.v_proj.bias',\n",
       " 'model.layers.14.self_attn.k_proj.bias',\n",
       " 'model.layers.14.self_attn.q_proj.bias',\n",
       " 'model.layers.14.self_attn.v_proj.bias',\n",
       " 'model.layers.15.self_attn.k_proj.bias',\n",
       " 'model.layers.15.self_attn.q_proj.bias',\n",
       " 'model.layers.15.self_attn.v_proj.bias',\n",
       " 'model.layers.16.self_attn.k_proj.bias',\n",
       " 'model.layers.16.self_attn.q_proj.bias',\n",
       " 'model.layers.16.self_attn.v_proj.bias',\n",
       " 'model.layers.17.self_attn.k_proj.bias',\n",
       " 'model.layers.17.self_attn.q_proj.bias',\n",
       " 'model.layers.17.self_attn.v_proj.bias',\n",
       " 'model.layers.18.self_attn.k_proj.bias',\n",
       " 'model.layers.18.self_attn.q_proj.bias',\n",
       " 'model.layers.18.self_attn.v_proj.bias',\n",
       " 'model.layers.19.self_attn.k_proj.bias',\n",
       " 'model.layers.19.self_attn.q_proj.bias',\n",
       " 'model.layers.19.self_attn.v_proj.bias',\n",
       " 'model.layers.2.self_attn.k_proj.bias',\n",
       " 'model.layers.2.self_attn.q_proj.bias',\n",
       " 'model.layers.2.self_attn.v_proj.bias',\n",
       " 'model.layers.20.self_attn.k_proj.bias',\n",
       " 'model.layers.20.self_attn.q_proj.bias',\n",
       " 'model.layers.20.self_attn.v_proj.bias',\n",
       " 'model.layers.21.self_attn.k_proj.bias',\n",
       " 'model.layers.21.self_attn.q_proj.bias',\n",
       " 'model.layers.21.self_attn.v_proj.bias',\n",
       " 'model.layers.22.self_attn.k_proj.bias',\n",
       " 'model.layers.22.self_attn.q_proj.bias',\n",
       " 'model.layers.22.self_attn.v_proj.bias',\n",
       " 'model.layers.23.self_attn.k_proj.bias',\n",
       " 'model.layers.23.self_attn.q_proj.bias',\n",
       " 'model.layers.23.self_attn.v_proj.bias',\n",
       " 'model.layers.24.self_attn.k_proj.bias',\n",
       " 'model.layers.24.self_attn.q_proj.bias',\n",
       " 'model.layers.24.self_attn.v_proj.bias',\n",
       " 'model.layers.25.self_attn.k_proj.bias',\n",
       " 'model.layers.25.self_attn.q_proj.bias',\n",
       " 'model.layers.25.self_attn.v_proj.bias',\n",
       " 'model.layers.26.self_attn.k_proj.bias',\n",
       " 'model.layers.26.self_attn.q_proj.bias',\n",
       " 'model.layers.26.self_attn.v_proj.bias',\n",
       " 'model.layers.27.self_attn.k_proj.bias',\n",
       " 'model.layers.27.self_attn.q_proj.bias',\n",
       " 'model.layers.27.self_attn.v_proj.bias',\n",
       " 'model.layers.3.self_attn.k_proj.bias',\n",
       " 'model.layers.3.self_attn.q_proj.bias',\n",
       " 'model.layers.3.self_attn.v_proj.bias',\n",
       " 'model.layers.4.self_attn.k_proj.bias',\n",
       " 'model.layers.4.self_attn.q_proj.bias',\n",
       " 'model.layers.4.self_attn.v_proj.bias',\n",
       " 'model.layers.5.self_attn.k_proj.bias',\n",
       " 'model.layers.5.self_attn.q_proj.bias',\n",
       " 'model.layers.5.self_attn.v_proj.bias',\n",
       " 'model.layers.6.self_attn.k_proj.bias',\n",
       " 'model.layers.6.self_attn.q_proj.bias',\n",
       " 'model.layers.6.self_attn.v_proj.bias',\n",
       " 'model.layers.7.self_attn.k_proj.bias',\n",
       " 'model.layers.7.self_attn.q_proj.bias',\n",
       " 'model.layers.7.self_attn.v_proj.bias',\n",
       " 'model.layers.8.self_attn.k_proj.bias',\n",
       " 'model.layers.8.self_attn.q_proj.bias',\n",
       " 'model.layers.8.self_attn.v_proj.bias',\n",
       " 'model.layers.9.self_attn.k_proj.bias',\n",
       " 'model.layers.9.self_attn.q_proj.bias',\n",
       " 'model.layers.9.self_attn.v_proj.bias'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(index_qwen2[\"weight_map\"].keys()) - set(index_llama3[\"weight_map\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.layers.28.input_layernorm.weight',\n",
       " 'model.layers.28.mlp.down_proj.weight',\n",
       " 'model.layers.28.mlp.gate_proj.weight',\n",
       " 'model.layers.28.mlp.up_proj.weight',\n",
       " 'model.layers.28.post_attention_layernorm.weight',\n",
       " 'model.layers.28.self_attn.k_proj.weight',\n",
       " 'model.layers.28.self_attn.o_proj.weight',\n",
       " 'model.layers.28.self_attn.q_proj.weight',\n",
       " 'model.layers.28.self_attn.v_proj.weight',\n",
       " 'model.layers.29.input_layernorm.weight',\n",
       " 'model.layers.29.mlp.down_proj.weight',\n",
       " 'model.layers.29.mlp.gate_proj.weight',\n",
       " 'model.layers.29.mlp.up_proj.weight',\n",
       " 'model.layers.29.post_attention_layernorm.weight',\n",
       " 'model.layers.29.self_attn.k_proj.weight',\n",
       " 'model.layers.29.self_attn.o_proj.weight',\n",
       " 'model.layers.29.self_attn.q_proj.weight',\n",
       " 'model.layers.29.self_attn.v_proj.weight',\n",
       " 'model.layers.30.input_layernorm.weight',\n",
       " 'model.layers.30.mlp.down_proj.weight',\n",
       " 'model.layers.30.mlp.gate_proj.weight',\n",
       " 'model.layers.30.mlp.up_proj.weight',\n",
       " 'model.layers.30.post_attention_layernorm.weight',\n",
       " 'model.layers.30.self_attn.k_proj.weight',\n",
       " 'model.layers.30.self_attn.o_proj.weight',\n",
       " 'model.layers.30.self_attn.q_proj.weight',\n",
       " 'model.layers.30.self_attn.v_proj.weight',\n",
       " 'model.layers.31.input_layernorm.weight',\n",
       " 'model.layers.31.mlp.down_proj.weight',\n",
       " 'model.layers.31.mlp.gate_proj.weight',\n",
       " 'model.layers.31.mlp.up_proj.weight',\n",
       " 'model.layers.31.post_attention_layernorm.weight',\n",
       " 'model.layers.31.self_attn.k_proj.weight',\n",
       " 'model.layers.31.self_attn.o_proj.weight',\n",
       " 'model.layers.31.self_attn.q_proj.weight',\n",
       " 'model.layers.31.self_attn.v_proj.weight'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(index_llama3[\"weight_map\"].keys()) - set(index_qwen2[\"weight_map\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
