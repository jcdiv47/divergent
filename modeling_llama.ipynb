{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.llama.modeling_llama import * # type: ignore\n",
    "# from modeling_llama_v1 import LlamaForCausalLM\n",
    "\n",
    "\n",
    "class LlamaForCausalLM_v1(LlamaForCausalLM):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show_cos_distance(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        layer_index: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Output the cosine distance between the input_hidden_states and output_hidden_states for a specific layer\n",
    "        \n",
    "        Args:\n",
    "            layer_index (int): The layer index to use\n",
    "\n",
    "        Returns:\n",
    "            cosine_distance (torch.Tensor): The cosine distance between the input_hidden_states and output_hidden_states\n",
    "        \"\"\"\n",
    "        assert layer_index > 0, \"layer 0 does not have input_hidden_states\"\n",
    "        outputs = self.model(input_ids, output_hidden_states=True)\n",
    "        input_hidden_states = outputs.hidden_states[layer_index - 1]\n",
    "        output_hidden_states = outputs.hidden_states[layer_index]\n",
    "        return F.cosine_similarity(input_hidden_states, output_hidden_states, dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show_topk_token(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        layer_index: int,\n",
    "        k: int = 10,\n",
    "    ) -> tuple[torch.Tensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Output the top k tokens for predicting the next token using a specific layer\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: The tokenizer to use\n",
    "            model: The model to use\n",
    "            input_text (str): The input text to use\n",
    "            layer_index (int): The layer index to use\n",
    "            k (int): The number of tokens to output\n",
    "\n",
    "        Returns:\n",
    "            values, tokens (tuple[torch.Tensor, list[str]]): A tuple containing the top k values and tokens\n",
    "        \"\"\"\n",
    "        assert k <= self.vocab_size, \"k cannot be greater than vocabulary size\"\n",
    "        outputs = self.model(input_ids, output_hidden_states=True)\n",
    "        values, indices = torch.topk(self.lm_head(outputs.hidden_states[layer_index])[:, -1, :], k, dim=-1)\n",
    "        return values, indices\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show_token_attention(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        layer_index: int,\n",
    "        token_a_index: int,\n",
    "        token_b_index: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Output the attention value between two tokens in the layer_index layer\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: The tokenizer to use\n",
    "            model: The model to use\n",
    "            input_text (str): The input text to use\n",
    "            layer_index (int): The layer index to use\n",
    "            token_a_index (int): The index of the first token\n",
    "            token_b_index (int): The index of the second token\n",
    "\n",
    "        Returns:\n",
    "            attention (torch.Tensor): The attention value between the two tokens\n",
    "        \"\"\"\n",
    "        # with `output_attentions=True`, calculation of attention falls back\n",
    "        # to the original implementation instead of `torch.nn.functional.scaled_dot_product_attention`\n",
    "        outputs = self.model(input_ids, output_attentions=True)\n",
    "        # select the attention for the layer and average over the heads\n",
    "        layer_attentions = outputs.attentions[layer_index].mean(dim=1)\n",
    "        # select the attention between the two tokens\n",
    "        return layer_attentions[:, token_a_index][:, token_b_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313cb08e207a47cdbf34f8aa50805922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM_v1(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = LlamaForCausalLM_v1.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\nWhat is the meaning of life?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3069, 0.3069, 0.5806, 0.6885, 0.5894, 0.6470, 0.6304, 0.6709, 0.6392,\n",
       "         0.6230, 0.6548, 0.6675, 0.5933, 0.4868, 0.5801, 0.5083, 0.5112, 0.5103,\n",
       "         0.3796, 0.3926, 0.4805, 0.3530, 0.5088, 0.5596, 0.6055, 0.5317, 0.5376,\n",
       "         0.6255, 0.6484]], dtype=torch.float16, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs.input_ids, output_hidden_states=True)\n",
    "# select the attention for the layer and average over the heads\n",
    "layer_index = 32\n",
    "torch.nn.functional.cosine_similarity(outputs.hidden_states[layer_index], outputs.hidden_states[layer_index - 1], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3069, 0.3069, 0.5806, 0.6885, 0.5894, 0.6470, 0.6304, 0.6709, 0.6392,\n",
       "         0.6230, 0.6548, 0.6675, 0.5933, 0.4868, 0.5801, 0.5083, 0.5112, 0.5103,\n",
       "         0.3796, 0.3926, 0.4805, 0.3530, 0.5088, 0.5596, 0.6055, 0.5317, 0.5376,\n",
       "         0.6255, 0.6484]], dtype=torch.float16, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(outputs.hidden_states[layer_index - 1], outputs.hidden_states[layer_index], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "`show_topk_token` method\n",
    "\"\"\"\n",
    "input_text = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\nWhat is the meaning of life?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "values, indices = model.show_topk_token(inputs.input_ids, -1, 10)\n",
    "tokenizer.batch_decode(indices.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the meaning of life?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=24,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "\n",
    "# The meaning of life! This is a question that has puzzled philosophers, theologians, scientists, and everyday people for centuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
